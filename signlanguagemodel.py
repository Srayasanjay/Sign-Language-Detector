# -*- coding: utf-8 -*-
"""SignLanguageModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q2vDgMEVDMAond6_k2CEnKw_CKb3fdQz
"""

!pip install mediapipe opencv-python tensorflow

from google.colab import files
uploaded = files.upload()

import cv2
import mediapipe as mp
import numpy as np
from google.colab.patches import cv2_imshow

mp_hands = mp.solutions.hands
hands = mp_hands.Hands()
mp_drawing = mp.solutions.drawing_utils

# Initialize a dictionary to store data
data = {'A': [], 'B': [], 'C': [],'D' :[],'E':[],'F':[],'G':[],'H':[],'I':[],'J':[],'K':[],'L':[],'BAD':[],'GOOD':[],'YES':[],'NO':[],'NAME':[],'HELLO':[]}  # Add more signs as needed

def collect_data_from_video(video_path, sign, max_frames=300):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Error opening video file: {video_path}")
        return

    frame_count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            print(f"End of video: {video_path}")
            break

        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        result = hands.process(frame_rgb)
        if result.multi_hand_landmarks:
            landmarks = result.multi_hand_landmarks[0].landmark
            landmarks = np.array([[lm.x, lm.y, lm.z] for lm in landmarks]).flatten()
            data[sign].append(landmarks)
            mp_drawing.draw_landmarks(frame, result.multi_hand_landmarks[0], mp_hands.HAND_CONNECTIONS)

        frame_count += 1
        if frame_count % 30 == 0:  # Show every 30th frame
            cv2_imshow(frame)

        if frame_count >= max_frames:  # Limit to max_frames frames per video
            print(f"Stopping data collection for {sign} after {max_frames} frames.")
            break

    cap.release()
    print(f"Collected {len(data[sign])} frames for sign: {sign}")

# Example to collect data for sign 'A' from an uploaded video
# Replace 'uploaded_video_A.mp4' with the actual file name of the uploaded video
collect_data_from_video('A.mov', 'A')
collect_data_from_video('B.mov', 'B')
collect_data_from_video('C.mov', 'C')
collect_data_from_video('D.mov', 'D')
collect_data_from_video('E.mov', 'E')
collect_data_from_video('F.mov', 'F')
collect_data_from_video('G.mov', 'G')
collect_data_from_video('H.mov', 'H')
collect_data_from_video('I.mov', 'I')
collect_data_from_video('J.mov', 'J')
collect_data_from_video('K.mov', 'K')
collect_data_from_video('L.mov', 'L')
collect_data_from_video('bad.mov','BAD')
collect_data_from_video('good.mov', 'GOOD')
collect_data_from_video('yes.mov', 'YES')
collect_data_from_video('no.mov', 'NO')
collect_data_from_video('name.mov', 'NAME')
collect_data_from_video('hello.mov', 'HELLO')

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten

# Prepare the dataset
X = []
y = []

for sign, landmarks in data.items():
    X.extend(landmarks)
    y.extend([sign] * len(landmarks))

X = np.array(X)
y = np.array(y)

# Encode labels
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Define the model
model = Sequential([
    Flatten(input_shape=(63,)),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(len(label_encoder.classes_), activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))

import tensorflow as tf
import joblib
from google.colab import files

# Assume 'model' is your trained model and 'label_encoder' is your trained label encoder

# Save the model
model.save('sign_language_model.h5')

# Save the label encoder
joblib.dump(label_encoder, 'label_encoder.pkl')

# Download the model file
files.download('sign_language_model.h5')

# Download the label encoder file
files.download('label_encoder.pkl')

def predict_sign_from_video(video_path):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Error opening video file: {video_path}")
        return

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        result = hands.process(frame_rgb)
        if result.multi_hand_landmarks:
            landmarks = result.multi_hand_landmarks[0].landmark
            landmarks = np.array([[lm.x, lm.y, lm.z] for lm in landmarks]).flatten()
            landmarks = np.expand_dims(landmarks, axis=0)
            prediction = model.predict(landmarks)
            sign = label_encoder.inverse_transform([np.argmax(prediction)])[0]
            cv2.putText(frame, sign, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)
        cv2_imshow(frame)

    cap.release()

# Predict sign from an uploaded video
# Replace 'uploaded_test_video.mp4' with the actual file name of the uploaded test video
predict_sign_from_video('test1.mov')